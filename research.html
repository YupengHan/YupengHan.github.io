<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-149845984-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-149845984-1');
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yupeng HAN's Research Interests </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">MENU</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="projects.html">Projects</a></div>
<div class="menu-item"><a href="courses.html">Courses</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yupeng HAN </h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/keywords2.png" alt="alt text" width="750px" height="307px" />&nbsp;</td>
<!-- <td align="left"><p> -->
<!-- <b>Perception Algorithms</b>:<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Two-Stage Region-based Object Detection<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Efficient Robotic Perception with Low Computation Consumption<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PointCloud-based 3D Object Detection<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generative Object estimation Approaches<br />
<b>Other Technical Capabilities</b>:<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[New] Graphics Rendering and Scene Reconstruction<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Search-Based Robotic Path Planning<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Algorithm Design and Optimization<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Statistic System Modeling<br /> -->
<!-- <b>Technical Skills</b>:<br />
<br /> -->
<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[New] Optimize Algorithm by Parallelization(CUDA)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[New] Graphics Rendering and Scene Reconstruction<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Efficient Robotic Perception with Low Computation Consumption<br /> -->
<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PointCloud-based 3D Object Detection<br /> -->
<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Statistic System Modeling<br /> -->
<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Search-Based Robotic Path Planning<br /> -->
<!-- </p> -->
</td></tr></table>

<h2>GPU-based Perception via Search for Vehicle Detection |Advisor: Prof. Maxim Likhachev</h2>
<p>Research Assistant, Search-Based Planning Lab, <b>The Robotics Institute, Carnegie Mellon University</b></p>
<ul>
<li><p>
<!-- Noticed the distinct advantages of detecting occluded objects with generative approaches that estimated the pose of every object in the scene by constantly reconstructing the scene and evaluating the difference between the reconstructed scene and the input scene. -->
Propose an accurate, robust, and real-time framework target to solve 3D vehicle detection problems.
<!-- Overcome the efficiency problem of 2D \& 3D feature fusion.
Learning-Free Framework that can accurately estimate vehicle 3D pose from the 2D bounding box with real-time performance. -->
</p>
<li><p>
Propose a novel way of aggregation the 2D & 3D information aiming to mitigate the occlusion caused double the viewpoints(LiDAR lens viewpoint and camera lens viewpoint)
<!-- Enhanced robotic perception performance under especially poor visual conditions by bridging the generative approaches with learning-based perception and prior knowledge embedded in the probabilistic graphical model. -->
</p>
</li>
<li>
<p>
<!-- Covered state-of-the-art probabilistic graphical models to explore the possibility of embedding prior knowledge into those models. -->
Learning-Free pipeline that can accurately estimate object 3D pose from 2D bounding box on RGB image.
</p>
</li>
<li>
<p>
Through GPU-based parallelization, the algorithm can obtain real-time processing capabilities.
<!-- Tested generative approaches with multiple evaluation functions and utilized prior data, such as number of cars on the road, to reduce the number of reconstructed scenes significantly; Experimented on 3D object detection task of KITTI Dataset. -->
</p>
</li>
<li>
<p>
Experiments on the KITTI dataset and achive good accuracy without the need of 3D pose annotation.
</p>
</li>
<li>
<p>[Ongoing Project] Going to submit to ICRA 2021</p>
</li>
<!-- Perception by Generation Videos 1274 × 720-->
<video width="637" height="360" controls>
  <source src="photos/PBG_outdoor.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

</ul>
<h2>Indoor 6-Dof Object Pose Estimation |Advisor: Prof. Maxim Likhachev</h2>
<p>Research Assistant, Search-Based Planning Lab, <b>The Robotics Institute, Carnegie Mellon University</b></p>
<ul>
<li><p> Estimate the object pose by using generative approach. First uniformly sample poses to discover the most likely pose, than adjust the poses with ICP alignment.</p>
</li>
<li>
  <p>
    <b>Open dataset(YCB) shows that our proposed PERCH 2.0 surpasses state-of-the-art 6Dof pose estimation methods with remarkable margins without the need for any ground truth pose annotation.</b>
  </p>
</li>
<li><p>Accepted by IROS 2020 on July 1st.</p></li>
<li><p>[<a href="https://arxiv.org/abs/2008.00326"> Paper</a>] | [<a href="https://sbpl-cruz.github.io/perception/"> Github</a>] | [<a href="https://www.youtube.com/watch?v=xNFWHj0rL6w"> Full Video Link</a>]</p></li>
<!-- <li><p></p></li>
<li><p></p></li> -->

<div>
    <img src="photos/pipeline.png" alt=""  width=579 height=320 />
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
    <!-- 3132 × 1614 -->
    <!-- 2D and 3D Feature Fusion Video -->
    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/0YDGnGosKqw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <video width="569" height="320" controls>
      <source src="photos/perch2.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
</div>


<!-- <li><p>Details will release after the submission.</p>
</li> -->
<!-- <div>
    <img src="photos/pipeline.png" alt=""  width=620 height=321 />
</div> -->
</ul>
<div class="infoblock">
<div class="blocktitle">Featured Publication:</div>
<div class="blockcontent">
<p>Agarwal Aditya, <b>Han Yupeng</b>, and Maxim Likhachev, "PERCH 2.0 : Fast and Accurate GPU-based Perception via Search for Object Pose Estimation"</p>
</div></div>
<h2>2D and 3D Feature Fusion for Autonomous Driving |Advisor: Prof. Cewu LU</h2>
<p>Research Assistant, Machine Vision and Intelligence Group, CS Department, SJTU</p>
<ul>
<li><p>Explored using a single model makes 3D detection in multi-scale objects; generated ideas to strengthen the 3D point cloud and extracted features by utilizing 2D RGB extracted features and dynamic anchor boxes&rsquo; size determined by 2D images.</p>
</li>
<li><p>Built the pipeline of the fusion network, including extracting different features from point clouds and RGB images, transforming 2D information to 3D proposal boxes, cropped key points and their 3D features inside the proposal boxes, and concatenated 3D features with 2D features, and performed post-processing.</p>
</li>
<li><p>Implemented a modified version of Faster-RCNN from scripts to feed in RGB images and output bounding boxes, classification labels, estimated depths, and proposal orientations in bird&ndash;eye view -– this project had over 6k lines of code.</p>
</li>
<li><p>Selected the sparse convolution network as the backbone of the 3D point cloud feature extractor due to its performance on ScanNet.</p>
</li>
<li><p>[<a href="https://github.com/YupengHan/FusionNet">Github</a>]</p>
</li>
<div>
    <img src="photos/fusion.png" alt=""  width=538 height=336 />
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
    <!-- 2D and 3D Feature Fusion Video -->
    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/0YDGnGosKqw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <video width="538" height="336" controls>
        <!-- 1152 × 720 -->
        <source src="photos/2D3Dfusion.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
</div>
</ul>

<h2>Depth Image Face Detection with Low Computation Power| Supervisor: Dr. Bo Wang, Stanford Ph.D., CTO at Deptrum</h2>
<ul>
<li><p>Independently Implemented a modified version of the multi-task cascade CNN model based on Caffe and Python from scripts, which solved the problem of facial detection on completed depth images; optimized the model to obtain 99.93% precision and over 97% recall.</p></li>
<!-- <li><p>Fine tuned the multi-task cascade CNN model and predicted the bounding box of human faces with the fine-tuned model on the RGB images corresponding to the depth images and saved the results as the labels for depth image face detection; transfered the RGB bounding box to depth images according to the camera model's intrinsic parameters.</p>
</li>
<li><p>Utilized median blur filters to address pepper-salt noises, applied histogram equalization to preprocessing near-infrared speckle pictures.</p>
</li> -->
<li><p>[<a href="https://github.com/YupengHan/Face-Detection-on-Depth-Images">GitHub</a>]</p>
</li>

<div>
    <img src="photos/deptrum.png" alt=""  width=560 height=315 />
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
    <!-- depth face demo -->
    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/f60uX4tkXAA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <video width="557" height="315" controls>
        <!-- 1274 × 720 -->
        <source src="photos/FaceDetection.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
</div>
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/iEiq6Xm_e1M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe> -->
</ul>
<h2>Data-Aware Algorithm to Solve Discrete Integration | Advisor: Prof. Yexiang Xue</h2>
<p>Machine Learning Group, CS Department, Purdue University</p>
<ul>
<li><p>Inspired by &ldquo;Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization&rdquo; and exploited a data-aware strategy to modify the original algorithm.</p>
</li>
<li><p>Generated a novel adaptive comparison strategy to reduce the expectation of computational complexity without loss of constant estimation guarantee and compared to the new algorithm with an imaginary &ldquo;optimal&rdquo; algorithm to provide a regret bound for the new algorithm.</p>
</li>
<li><p>[<a href="algorithm_design.pdf">Manuscripts</a>] | [<a href="https://drive.google.com/open?id=1ebdHFkPVb7xPHdFYmMZG4JlAzdwkPzxv">Presentation Slides</a>]</p>
</li>
<!-- GIF PI1 -->
<div class="image"/><br />
<div>
    <img src="gifs/PI1.gif" alt=""  width=550 height=309 />
    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
    <img src="gifs/PI2.gif" alt=""  width=550 height=309 />
</div>
</ul>
<h2>Modeling and Analysis of Complex System | Advisor: Prof. Jitesh Panchal</h2>
<p>Design Engineering Lab at Purdue(DELP), ME Department, Purdue University</p>
<ul>
<li><p>Solved the difficulty of service seekers when faced with a large number of service providers, also addressed the drawbacks of the First In First Out (FIFO) matching mechanism by developed a stable matching system based on utility theory to generate the preference lists of service providers and service seekers. The matching mechanism was accomplished based on different utility interests. Searched for the optimal matching frequency using the provided matching.</p>
</li>
<li><p>Implemented the matching algorithm based on MATLAB and simulated service seekers arrive as a Poisson process with a fixed number of service providers offering resources. The service providers could only serve for one service seekers at one time.</p>
</li>
<li><p>[<a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2018/51722/V01AT02A024/273503">Paper</a>]</p>
</li>
</ul>
<div class="infoblock">
<div class="blocktitle">Featured Publication:</div>
<div class="blockcontent">
<p>Thekinen J., <b>Han Yupeng</b>, and Panchal J. H., "Designing market thickness and optimal frequency of multi-period stable matching in CBDM" <br />
ASME 2018 International Design Engineering Technical Conferences Computers and Information in Engineering Conference. [<a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2018/51722/V01AT02A024/273503">pdf</a>] <br /></p>
</div></div>



<!-- <h2>Social Network Analysis | Advisor: Prof. David F. Gleich</h2>
<p>Social Network Analysis | Advisor: Prof. David F. Gleich, CS Department, Purdue University</p>
<ul>
<li><p>Read Ego-splitting from previous paper which is a highly scalable and flexible framework that reduces the complex overlapping clustering problem to a more straightforward and more amenable non-overlapping partitioning problem.</p>
</li>
<li><p>Re-accomplished the Ego-splitting framework in Julia. The new framework could handle a large graph (millions of edges) within a few (less than 10) minutes.</p>
</li>
<li><p>Demo of how Ego-splitting works, nodes stands for people, and edges stands for relationship. Nodes #1 is Yupeng HAN, nodes #2, #3, #4 are Yupeng's shoolmates, nodes #8, #9, #10 are Yupeng's co-workers and nodes #5, #6, #7 are Yupeng's family members. To better analysis the social network, we can divide Yupeng in to three nodes, #1, #11, #12 to represent Yupeng's working, studying, home characters. Following are the demo and the result of the efficiency. </p>
</li>
<li><p>[<a href="https://github.com/YupengHan/Ego-splitting-in-Julia">GitHub</a>]</p>
</li>
</ul>
<table class="imgtable"><tr><td>
<img src="photos/ego_split_demo.png" alt="alt text" width="930px" height="250px" />&nbsp;</td>
<td align="left"></td></tr></table>
<table class="imgtable"><tr><td>
<img src="photos/ego_split_result.png" alt="alt text" width="930px" height="290px" />&nbsp;</td>
<td align="left"></td></tr></table> -->
</td>
</tr>
</table>
</body>
</html>
